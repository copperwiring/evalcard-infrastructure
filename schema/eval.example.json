{
    "schema_version": "0.0.1",
    "evaluation_id": "eval_001",
    "model": {
        "model_info": {
            "name": "Llama-2-7b-hf"
        },
        "configuration": {
            "context_window": 4096
        },
        "inference_settings": {
            "quantization": {
                "bit_precision": "float16",
                "method": "None"
            }
        }
    },
    "prompt_config": {
        "prompt_class": "MultipleChoice"
    },
    "instance": {
        "task_type": "classification",
        "raw_input": "What is 2+2?",
        "language": "en",
        "sample_identifier": {
            "dataset_name": "math_basic",
            "hf_repo": "example/math",
            "hf_split": "test",
            "hf_index": 1
        },
        "classification_fields": {
            "question": "What is 2+2?",
            "choices": [
                {
                    "id": "A",
                    "text": "3"
                },
                {
                    "id": "B",
                    "text": "4"
                }
            ],
            "ground_truth": {
                "id": "B",
                "text": "4"
            }
        }
    },
    "output": {
        "response": "B"
    },
    "evaluation": {
        "evaluation_method": {
            "method_name": "label_only_match",
            "description": "Compares only the choice identifier/label to evaluate the response."
        },
        "ground_truth": "B",
        "score": 1.0
    }
}